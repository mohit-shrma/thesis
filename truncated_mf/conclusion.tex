\section{Conclusion} 
\label{ch:tmf:conclusion}

In this chapter, we learned that the user or items with few ratings may not have 
sufficient ratings to estimate the low-rank models accurately thereby
leading to an error in predictions and thus affecting item recommendations.
Based on these insights we presented \TMF in Section~\ref{ch:tmf:drop_method}, which considers the frequency of
both the user and the item to select a subset of ranks to estimate
the rating of the user on the item accurately. 
The experiments on real datasets show that \TMF outperforms the
state-of-the-art \MF method
for rating predictions for the users and the items having few ratings in the
user-item rating matrix.

\iffalse
It will be interesting to evaluate the ranking performance of
the recommendations generated by the proposed approaches. Additionally, we can
use ranking loss functions~\cite{shi2012climf,rendle2009bpr} instead of square error loss function to estimate
the low-rank models for better ranking performance.
Furthermore, we can explore the usage of different non-linear activation
functions other than sigmoid function, e.g.,  hyperbolic tangent (Tanh) and
rectified linear units (ReLu), in \TMF as different functions might be suited for
datasets with distinctive characteristics. Similarly, we can investigate usage
of different probability distributions other than the Poisson distribution in TMF + Dropout
method. 
We can also leverage the derived insight in Section~\ref{ch:tmf:freqanal}, i.e., only fewer ranks
are estimated accurately for users or items with few ratings,  to modify
existing locality-based  matrix completion methods~\cite{lee2013local,
lee2014local,chen2015wemarec} by using lower ranks for
the sparse part and higher ranks for the dense part of the user-item rating
matrix.
\fi

